% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

Video-conferencing platforms have become more than just communication channels. 
They are also used for conducting user studies and experiments for research and educational purposes.
Due to the easy accessability of having the video data from these platforms and the surging of ongoing research about facial extraction technique,
Facial Action Units (AUs) data are now possible to obtain by utilizing open source tools, such as OpenFace~\parencite{openface} and PyFeat~\parencite{py-feat}.
There are several interesting applications that will be greatly benefited from
the extracted AUs. One of them is synchrony measurement, which lies in both computer science and psychology area.

Obtaining AUs data is pretty straightforward for people with technology background. However, it is not that easy to get them running for non-tech people. 
Let alone the time for understanding and installing the tool, the extraction process itself also takes quite sometime.
The integration of the facial extraction tools into a video-conferencing platform is motivated by a desire to fast-forward that long process 
and have everyone in any research background could get the AUs data with less hassle.
The first step of this integration is already implemented on a video-conferencing platform called experimental-hub~\parencite{experimental-hub},
using OpenFace as the facial extraction tool that is run as a real-time filter on the video stream.

As a representative case study, OpenFace serves as a powerful tool in this field because of its capability to extract variety of facial data from image sequences or video.
OpenFace is built as a standalone toolbox which consists of multiple facial and landmark detection technology inside 
so integrating them with the experimental hub is beneficial but also gave us a lot of drawbacks. 
Running a machine learning project has always been a heavy-computation task for a machine, which OpenFace does the same.
 Moreover, if we want to apply this real-time filter, we need to run it in multiple instances simultaneously for each user in the experimental hub.
It is very desirable to have it running without slowing down the video stream and not losing the frame-by-frame AUs data of the video.
Thus, this thesis aims to minimize data lost and run the video stream in a normal fps, so we optimized the existing real-time AUs filter 
and have the post-processing feature as a backup plan.
The case study with OpenFace, coupled with the queue system 
facilitated by RabbitMQ for the asnychronous exchanging frames process.
(add more sentences on why we built the post-processing)
