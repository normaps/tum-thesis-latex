@article{py-feat,
  author     = {Jin Hyun Cheong and
                Tiankang Xie and
                Sophie Byrne and
                Luke J. Chang},
  title      = {Py-Feat: Python Facial Expression Analysis Toolbox},
  journal    = {CoRR},
  volume     = {abs/2104.03509},
  year       = {2021},
  url        = {https://arxiv.org/abs/2104.03509},
  eprinttype = {arXiv},
  eprint     = {2104.03509},
  timestamp  = {Tue, 13 Apr 2021 16:46:17 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-03509.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@article{ar-filter-on-social-media,
  abstract   = {Augmented reality (AR) filters are a popular social media feature affording users a variety of visual effects. Despite their widespread use, no research to date has examined either `why' people use them (i.e., motivations) or `how' their usage makes people feel (i.e., well-being effects). Through the uses and gratifications theory supported by a sequential mixed-method approach (interviews N = 10 and survey N = 536), we provide three overarching contributions. First, based on prior literature and a qualitative study, we identify nine motivations that can potentially drive AR face filter usage on Instagram. Our survey indicates that seven of those motivations (e.g., creative content curation, social interactions) are significant drivers of usage behaviours, while two (true self-presentation and silliness) did not have a significant impact. Second, we provide nuanced insights into the multi-faceted nature of the self-presentation motives underpinning AR face filter use (ideal, true and transformed self-presentation). Lastly, we show filter usage can have both positive and negative well-being effects depending on the underlying motivation. The results offer important implications for policymakers, site designers and social media managers.},
  author     = {Ana Javornik and Ben Marder and Jennifer Brannon Barhorst and Graeme McLean and Yvonne Rogers and Paul Marshall and Luk Warlop},
  doi        = {https://doi.org/10.1016/j.chb.2021.107126},
  issn       = {0747-5632},
  journal    = {Computers in Human Behavior},
  keywords   = {Augmented reality, Face filter, Social media, Uses and gratifications, Well-being, Self-presentation},
  pages      = {107126},
  title      = {`What lies behind the filter?' Uncovering the motivations for using augmented reality (AR) face filters on social media and their effect on well-being},
  url        = {https://www.sciencedirect.com/science/article/pii/S0747563221004490},
  volume     = {128},
  year       = {2022},
  bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0747563221004490},
  bdsk-url-2 = {https://doi.org/10.1016/j.chb.2021.107126}
}
@inproceedings{openface,
  author    = {Baltrusaitis, Tadas and Zadeh, Amir and Lim, Yao Chong and Morency, Louis-Philippe},
  booktitle = {2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)},
  title     = {OpenFace 2.0: Facial Behavior Analysis Toolkit},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {59-66},
  doi       = {10.1109/FG.2018.00019}
}
@inproceedings{experimental-hub,
  author    = {Eghtebas, Chloe and Liebald, Alexander and Pospelova, Maria and Manjunath, Ashika and Geheeb, Julian and Puspitasari, Norma and Ward, Jamie A and Klinker, Gudrun},
  title     = {An Experimental Video Conference Platform to Bridge the Gap Between Digital and In-Person Communication},
  year      = {2023},
  isbn      = {9798400702006},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3594739.3610686},
  doi       = {10.1145/3594739.3610686},
  abstract  = {With many contemporary video conferencing platforms available, there is still a need for platforms that afford a researcher workflow to conduct controlled online experiments. We have developed an open source experimental video conferencing platform that enables researchers to design and conduct remote experiments. Our platform provides a high level of control over the user interface and video streams, which is essential for studying the differences between remote and in-person social interactions. We give an overview of our platform's usage and architecture and conduct a take-home study (N=9) to evaluate how accessible our system is to potential new contributors. We also follow up with an initial evaluation of technical performance bottlenecks for when our experimental platform is deployed, and show that the computational resources increases per each video stream as well as the type of filters applied to each participant. We end with a short discussion on next steps and the experimental hub's potential to be extended as a sandbox for testing browser based augmented reality (WebAR) filters to be adopted in interdisciplinary experimental procedures.},
  booktitle = {Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing \& the 2023 ACM International Symposium on Wearable Computing},
  pages     = {35-39},
  numpages  = {5},
  keywords  = {WebAR, Video Conferencing, Remote User Studies},
  location  = {<conf-loc>, <city>Cancun, Quintana Roo</city>, <country>Mexico</country>, </conf-loc>},
  series    = {UbiComp/ISWC '23 Adjunct}
}
@inproceedings{blur-privacy-ar,
  author    = {Sabra, Mohd and Maiti, Anindya and Jadliwala, Murtuza},
  booktitle = {2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
  title     = {Background Buster: Peeking through Virtual Backgrounds in Online Video Calls},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {522-533},
  doi       = {10.1109/DSN53405.2022.00058}
}

@article{tracked,
  abstract   = {Online classrooms has seen an unprecedented spike in demand since the COVID-19 pandemic. This presents a challenge for educators to observe students' body language, which is more easily done in a face-to-face setting. This paper presents TrackEd, a tool based on emotion classification that detects body language in form of emotions expressed facially. The tool extracts image frames of attendees on live online meeting stream, and tracks their facial emotion. This is facilitated by a convolutional neural network model trained on 40,254 images categorised into seven facial expressions. The complete source code is freely available on Github entitled `TrackEd'.},
  author     = {Jamie McGrath and Nonso Nnamoko},
  doi        = {https://doi.org/10.1016/j.simpa.2023.100560},
  issn       = {2665-9638},
  journal    = {Software Impacts},
  keywords   = {Facial expression recognition, Deep learning, Online classroom, Emotion detection, Artificial intelligence, E-meeting platform, Affective computing},
  pages      = {100560},
  title      = {TrackEd: An emotion tracking tool for e-meeting platforms},
  url        = {https://www.sciencedirect.com/science/article/pii/S2665963823000970},
  volume     = {17},
  year       = {2023},
  bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2665963823000970},
  bdsk-url-2 = {https://doi.org/10.1016/j.simpa.2023.100560}
}
